{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, Dropout, BatchNormalization, LSTM\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim.models import Word2Vec\nfrom tqdm.notebook import tqdm\n\nword_lemmatizer = WordNetLemmatizer()\neng_stop = set(stopwords.words('english'))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":5,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.11.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.2)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\n\nbert_model_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)","execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b0b964378d547869f7e6fb5659078a1"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# constants\nBASE_PATH = '../input/jigsaw-toxic-comment-classification-challenge/'\nTRAIN_PATH = 'train.csv.zip'\nTEST_PATH = 'test.csv.zip'\nLABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nNUM_CLASSES = 6\nMAX_WORDS=10000\nMAX_LEN=128","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load train test dataframe\ntrain = pd.read_csv(f\"{BASE_PATH}{TRAIN_PATH}\")\ntest = pd.read_csv(f\"{BASE_PATH}{TEST_PATH}\")","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = train['comment_text'].fillna(\"CVxTz\").to_list()\ntrain_labels = train[LABELS].values\ntest_text = test['comment_text'].fillna(\"CVxTz\").to_list()","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(\"'\", \"\", text)\n    words = re.split(r'\\W+', text)\n    text = \" \".join(words)\n    text = re.sub(\"\\d+\", \"\", text)\n    text = \" \".join(text.split())\n    return text.strip()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_text = list(map(clean_text, train_text))\nclean_test_text = list(map(clean_text, test_text))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    words = [word for word in text.split() if word not in eng_stop]\n    return \" \".join(words)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize(text):\n    words = text.split()\n    lemmatized_words = list(map(word_lemmatizer.lemmatize, words))\n    return \" \".join(lemmatized_words)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_text = list(map(remove_stopwords, clean_train_text))\nclean_train_text = list(map(lemmatize, clean_train_text))\n\nclean_test_text = list(map(remove_stopwords, clean_test_text))\nclean_test_text = list(map(lemmatize, clean_test_text))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = [*clean_train_text, *clean_test_text]\n#w2v_model = train_word2vec(corpus)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids = tokenize_sentences(clean_train_text, tokenizer)\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, padding=\"post\")\nattention_masks = create_attention_masks(input_ids)","execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=159571.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194fdb474e32480980df7d4166c078ab"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import TFBertModel, BertConfig","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    # BERT encoder\n    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n    \n    # multi-label classification model\n    input_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n    embedding = encoder(input_ids, attention_mask=attention_mask)[1]\n    \n    dense_1 = Dense(64, activation='relu')(embedding)\n    dense_1 = Dropout(0.1)(dense_1)\n    dense_1 = BatchNormalization()(dense_1)\n    \n    output = Dense(6, activation='sigmoid')(dense_1)\n    \n    model = keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n    return model","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nmodel.summary()","execution_count":29,"outputs":[{"output_type":"stream","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_6 (InputLayer)            [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ninput_7 (InputLayer)            [(None, 128)]        0                                            \n__________________________________________________________________________________________________\ntf_bert_model_3 (TFBertModel)   ((None, 128, 768), ( 109482240   input_6[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 64)           49216       tf_bert_model_3[0][1]            \n__________________________________________________________________________________________________\ndropout_149 (Dropout)           (None, 64)           0           dense_2[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 64)           256         dropout_149[0][0]                \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 6)            390         batch_normalization_1[0][0]      \n==================================================================================================\nTotal params: 109,532,102\nTrainable params: 109,531,974\nNon-trainable params: 128\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.device('/gpu:0'):\n    history = model.fit([input_ids, attention_masks], train_labels, epochs=1, batch_size=64, validation_split=0.2)","execution_count":33,"outputs":[{"output_type":"stream","text":"1995/1995 [==============================] - 1922s 963ms/step - loss: 0.1737 - accuracy: 0.8681 - val_loss: 0.1491 - val_accuracy: 0.9941\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input_ids = tokenize_sentences(clean_test_text, tokenizer)\ntest_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, padding=\"post\")\ntest_attention_masks = create_attention_masks(test_input_ids)","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=153164.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d20d42d0304ce9a21805ba48bce0d4"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = model.predict([test_input_ids, test_attention_masks])\n\nres = []\nids = test[\"id\"].to_list()\nfor idx, label in zip(ids, test_labels):\n    res.append([idx, *label])\n\nout = pd.DataFrame(res, columns=[\"id\", *LABELS])\nout.to_csv(\"out.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}