{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport keras\nimport tensorflow as tf\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, Dropout, BatchNormalization, LSTM\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim.models import Word2Vec\n\nword_lemmatizer = WordNetLemmatizer()\neng_stop = set(stopwords.words('english'))","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.test.gpu_device_name())","execution_count":23,"outputs":[{"output_type":"stream","text":"/device:GPU:0\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# constants\nBASE_PATH = '../input/jigsaw-toxic-comment-classification-challenge/'\nTRAIN_PATH = 'train.csv.zip'\nTEST_PATH = 'test.csv.zip'\nLABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nNUM_CLASSES = 6\nMAX_WORDS=10000","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load train test dataframe\ntrain = pd.read_csv(f\"{BASE_PATH}{TRAIN_PATH}\")\ntest = pd.read_csv(f\"{BASE_PATH}{TEST_PATH}\")","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = train['comment_text'].to_list()\ntrain_labels = train[LABELS].values\ntest_text = test['comment_text'].to_list()","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(\"'\", \"\", text)\n    words = re.split(r'\\W+', text)\n    text = \" \".join(words)\n    text = re.sub(\"\\d+\", \"\", text)\n    text = \" \".join(text.split())\n    return text.strip()","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_text = list(map(clean_text, train_text))\nclean_test_text = list(map(clean_text, test_text))","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    words = [word for word in text.split() if word not in eng_stop]\n    return \" \".join(words)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize(text):\n    words = text.split()\n    lemmatized_words = list(map(word_lemmatizer.lemmatize, words))\n    return \" \".join(lemmatized_words)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenizer(texts, max_words):\n    tokenizer = Tokenizer(num_words=max_words)\n    tokenizer.fit_on_texts(texts)\n    return tokenizer","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_text = list(map(remove_stopwords, clean_train_text))\nclean_train_text = list(map(lemmatize, clean_train_text))\n\nclean_test_text = list(map(remove_stopwords, clean_test_text))\nclean_test_text = list(map(lemmatize, clean_test_text))","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_word2vec(corpus):\n    sentences = [text.split() for text in corpus]\n    word2vec_model = Word2Vec(min_count=1, size=64)\n    word2vec_model.build_vocab(sentences)\n    word2vec_model.train(sentences, total_examples=word2vec_model.corpus_count, epochs=20)\n    return word2vec_model","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = [*clean_train_text, *clean_test_text]\n#w2v_model = train_word2vec(corpus)","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenizer = tokenizer(corpus, MAX_WORDS)","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq = tokenizer.texts_to_sequences(clean_train_text)\ntest_seq = tokenizer.texts_to_sequences(clean_test_text)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_max_len(sequences):\n    lengths = list(map(len, sequences))\n    return max(lengths)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = find_max_len([*train_seq, *test_seq])\ntrain_seq = pad_sequences(train_seq, maxlen=max_len, padding=\"post\")\ntest_seq = pad_sequences(test_seq, maxlen=max_len, padding=\"post\")","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nembedding_matrix = np.zeros((MAX_WORDS, 64))\nfor word, i in word_index.items():\n    if i < MAX_WORDS:\n        if word not in w2v_model:\n            continue\n        embedding = w2v_model[word]\n        embedding_matrix[i] = embedding\n\nidx=0\nfor row in embedding_matrix:\n    norm = np.linalg.norm(row)\n    if norm !=0:\n        embedding_matrix[idx]/=norm\n    idx+=1","execution_count":116,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n  \"\"\"\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n  import sys\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(MAX_WORDS, 64, input_length=max_len))\n#model.add(BatchNormalization())\n#model.add(LSTM(64, dropout=0.1, return_sequences=True, activation='relu'))\nmodel.add(LSTM(32, dropout=0.1, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(6, activation='sigmoid'))","execution_count":117,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = False\nmodel.summary()","execution_count":118,"outputs":[{"output_type":"stream","text":"Model: \"sequential_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_7 (Embedding)      (None, 1426, 64)          640000    \n_________________________________________________________________\nlstm_7 (LSTM)                (None, 32)                12416     \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 32)                128       \n_________________________________________________________________\ndense_7 (Dense)              (None, 6)                 198       \n=================================================================\nTotal params: 652,742\nTrainable params: 12,678\nNon-trainable params: 640,064\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.device('/gpu:0'):    \n    model.fit(train_seq, train_labels, epochs=1, batch_size=64, validation_split=0.2)","execution_count":123,"outputs":[{"output_type":"stream","text":"1995/1995 [==============================] - 4783s 2s/step - loss: 0.1699 - accuracy: 0.9942 - val_loss: 0.1410 - val_accuracy: 0.9941\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels = model.predict(test_seq)","execution_count":127,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.save('word2vec-model-1')","execution_count":126,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = test[\"id\"].to_list()\nres = []\nfor idx, label in zip(ids, test_labels):\n    res.append([idx, *label])","execution_count":131,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = pd.DataFrame(res, columns=[\"id\", *LABELS])","execution_count":132,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out.to_csv(\"out.csv\", index=False)","execution_count":133,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}